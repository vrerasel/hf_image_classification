{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023293f4-72d2-4044-8e65-0bf979dc8d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vrera\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.notebook import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfa9056-5014-4853-83a8-48e6013fadf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\3cf4207b958eade893a2f1618cf062b8.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train\\37698901280c871f426d40afe5c373cd.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train\\20e7b30026001cbfe0b5c0ee16c9ff56.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>train\\a1bc8ea546206ee8fc0f1836fda9a5c1.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\54eb76914b84db8a0d56f98125abf588.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\07b420b4fe265b4ed918b46435c025d7.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>6</td>\n",
       "      <td>train\\2d1c5918357bbdd729bf79085e55d35e.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>0</td>\n",
       "      <td>train\\1531efa9f8687e390adf780355acd606.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>1</td>\n",
       "      <td>train\\2b15eaef0ce9b57b6570709f95a4bea4.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>6</td>\n",
       "      <td>train\\d1fec8a6b6be63534c37f0a26e94c7e8.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_id                                  image_name\n",
       "0             5  train\\3cf4207b958eade893a2f1618cf062b8.JPG\n",
       "1             2  train\\37698901280c871f426d40afe5c373cd.JPG\n",
       "2             0  train\\20e7b30026001cbfe0b5c0ee16c9ff56.JPG\n",
       "3             2  train\\a1bc8ea546206ee8fc0f1836fda9a5c1.JPG\n",
       "4             5  train\\54eb76914b84db8a0d56f98125abf588.JPG\n",
       "...         ...                                         ...\n",
       "28010         5  train\\07b420b4fe265b4ed918b46435c025d7.JPG\n",
       "28011         6  train\\2d1c5918357bbdd729bf79085e55d35e.JPG\n",
       "28012         0  train\\1531efa9f8687e390adf780355acd606.JPG\n",
       "28013         1  train\\2b15eaef0ce9b57b6570709f95a4bea4.JPG\n",
       "28014         6  train\\d1fec8a6b6be63534c37f0a26e94c7e8.JPG\n",
       "\n",
       "[28015 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data['image_name'] = data['image_name'].apply(lambda x: os.path.join('train', x))\n",
    "#data['image'] = data['image_name'].apply(lambda x: os.path.join('train', x))\n",
    "#data['label'] = data['class_id']\n",
    "data = data.drop(columns=['unified_class'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbb0b7b-e4cc-43fb-834f-8da552633405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887cfe9aced44a6ca75de06f2a399752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\3cf4207b958eade893a2f1618cf062b8.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train\\37698901280c871f426d40afe5c373cd.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train\\20e7b30026001cbfe0b5c0ee16c9ff56.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>train\\a1bc8ea546206ee8fc0f1836fda9a5c1.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\54eb76914b84db8a0d56f98125abf588.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28009</th>\n",
       "      <td>5</td>\n",
       "      <td>train\\07b420b4fe265b4ed918b46435c025d7.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>6</td>\n",
       "      <td>train\\2d1c5918357bbdd729bf79085e55d35e.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>0</td>\n",
       "      <td>train\\1531efa9f8687e390adf780355acd606.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>1</td>\n",
       "      <td>train\\2b15eaef0ce9b57b6570709f95a4bea4.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>6</td>\n",
       "      <td>train\\d1fec8a6b6be63534c37f0a26e94c7e8.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28014 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_id                                  image_name\n",
       "0             5  train\\3cf4207b958eade893a2f1618cf062b8.JPG\n",
       "1             2  train\\37698901280c871f426d40afe5c373cd.JPG\n",
       "2             0  train\\20e7b30026001cbfe0b5c0ee16c9ff56.JPG\n",
       "3             2  train\\a1bc8ea546206ee8fc0f1836fda9a5c1.JPG\n",
       "4             5  train\\54eb76914b84db8a0d56f98125abf588.JPG\n",
       "...         ...                                         ...\n",
       "28009         5  train\\07b420b4fe265b4ed918b46435c025d7.JPG\n",
       "28010         6  train\\2d1c5918357bbdd729bf79085e55d35e.JPG\n",
       "28011         0  train\\1531efa9f8687e390adf780355acd606.JPG\n",
       "28012         1  train\\2b15eaef0ce9b57b6570709f95a4bea4.JPG\n",
       "28013         6  train\\d1fec8a6b6be63534c37f0a26e94c7e8.JPG\n",
       "\n",
       "[28014 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[len(np.array(PIL.Image.open(image)).shape) == 3 for image in tqdm(data['image_name'])]].reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3650eda6-096d-45b6-9adb-f5dc4d8d65d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([10, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/resnet-50\"\n",
    "#model_name = \"google/vit-base-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name, num_labels=len(set(data['class_id'])), ignore_mismatched_sizes=True)\n",
    "#processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "#model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec68382-41ad-4950-bfc5-cf857b374ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(batch):\n",
    "    inputs = processor(batch['image_name'], return_tensors='pt')\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].squeeze(0) #if dataset.map\n",
    "    #inputs['labels'] = batch['class_id'] #if dataset.with_transform (low memory)\n",
    "    #print(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499ea91f-71b5-40a5-89ce-3841cc84be02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3a169a2f3546aeb45e4ef12a853d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/28014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4be89bbb0f414094f2836077ec6015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/28014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb78ad70a89448f93f8bf4a3c0e7a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 14007\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'pixel_values'],\n",
       "        num_rows: 14007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.rename_column('class_id', 'labels')\n",
    "dataset = dataset.class_encode_column('labels')\n",
    "dataset = dataset.cast_column('image_name', Image())\n",
    "dataset = dataset.map(transforms, remove_columns=['image_name']) # if enough memory\n",
    "dataset = dataset.with_format('torch') # if dataset.map\n",
    "#dataset = dataset.with_transform(transforms) # if low memory\n",
    "dataset = dataset.train_test_split(test_size=0.5, stratify_by_column='labels', seed=42)\n",
    "del data\n",
    "gc.collect()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578b2e10-1f50-4d24-99ca-641923b7ffb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(5),\n",
       " 'pixel_values': tensor([[[ 0.3994,  0.3823,  0.3481,  ...,  0.2796,  0.1939, -0.1657],\n",
       "          [ 0.3138,  0.2796,  0.3652,  ...,  0.2453,  0.0912, -0.0116],\n",
       "          [ 0.3994,  0.4166,  0.4508,  ...,  0.3309,  0.0741,  0.0741],\n",
       "          ...,\n",
       "          [ 0.5707,  0.5878,  0.6392,  ...,  2.0092,  1.9407,  1.9407],\n",
       "          [ 0.6563,  0.7419,  0.9132,  ...,  2.0948,  2.0434,  1.8893],\n",
       "          [ 0.7077,  0.7762,  0.8104,  ...,  2.1119,  2.0605,  1.9407]],\n",
       " \n",
       "         [[ 0.5378,  0.5203,  0.4853,  ...,  0.4153,  0.3277, -0.0399],\n",
       "          [ 0.4503,  0.4153,  0.5028,  ...,  0.3803,  0.2227,  0.1176],\n",
       "          [ 0.5378,  0.5553,  0.5903,  ...,  0.4678,  0.2052,  0.2052],\n",
       "          ...,\n",
       "          [ 0.7129,  0.7304,  0.7829,  ...,  2.1835,  2.1134,  2.1134],\n",
       "          [ 0.8004,  0.8880,  1.0630,  ...,  2.2710,  2.2185,  2.0609],\n",
       "          [ 0.8529,  0.9230,  0.9580,  ...,  2.2885,  2.2360,  2.1134]],\n",
       " \n",
       "         [[ 0.7576,  0.7402,  0.7054,  ...,  0.6356,  0.5485,  0.1825],\n",
       "          [ 0.6705,  0.6356,  0.7228,  ...,  0.6008,  0.4439,  0.3393],\n",
       "          [ 0.7576,  0.7751,  0.8099,  ...,  0.6879,  0.4265,  0.4265],\n",
       "          ...,\n",
       "          [ 0.9319,  0.9494,  1.0017,  ...,  2.3960,  2.3263,  2.3263],\n",
       "          [ 1.0191,  1.1062,  1.2805,  ...,  2.4831,  2.4308,  2.2740],\n",
       "          [ 1.0714,  1.1411,  1.1759,  ...,  2.5006,  2.4483,  2.3263]]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset = dataset['train'], dataset['test']\n",
    "del dataset\n",
    "gc.collect()\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b25786-5871-4b2f-a19c-a5b24f1c7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='cache',\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    #weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    #fp16=True #float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a52a3a-0713-4dce-abd6-bddaf24b59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "792719db-803e-4939-bfc3-6588d90fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(output):\n",
    "    logits, labels = output\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db26e-3f84-4ee5-89a2-4fde6f56a08d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "weight = torch.tensor(compute_class_weight(class_weight='balanced', classes=np.unique(train_dataset['labels']), y=train_dataset['labels'].numpy()), dtype=torch.float, device=model.device)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        #loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([8.0, 1.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "trainer = CustomTrainer( #Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,#train_dataset,#train_dataset,#['train'][:10],\n",
    "    eval_dataset=eval_dataset,#eval_dataset#['test'],\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e5df65-c264-4438-954e-1c8dad0b9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,#train_dataset,#train_dataset,#['train'][:10],\n",
    "    eval_dataset=eval_dataset,#eval_dataset#['test'],\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e03db44-5b94-4eb6-b953-6e7808e11c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1095/1095 13:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>0.352678</td>\n",
       "      <td>0.789173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.190422</td>\n",
       "      <td>0.898799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.150967</td>\n",
       "      <td>0.923571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.136635</td>\n",
       "      <td>0.932440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.137192</td>\n",
       "      <td>0.934048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90db3b2-cae2-40c4-a7cc-462f26cb5cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a55fade201f4cd099a5ec19a4eaf002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test\\cc27b9b56583a615fb8501e352402eb9.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test\\87872711fe672676fd34a97e997f9c47.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test\\424aa1aa8eb5bbdd07275f88077bc86c.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test\\c5537eaa60525efd7bad4a5560607e83.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test\\e9f15b67ca49453e281b2b4f245eac13.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12953</th>\n",
       "      <td>test\\028668e733cd17ec9b9f1c7e2c657b36.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12954</th>\n",
       "      <td>test\\eb1f1152941fdfdd50ff9954010e622a.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12955</th>\n",
       "      <td>test\\bfd2dde9f4a5753c9f85b2a93bee9c03.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>test\\2eaf9c794958a93bb9984441fd5d7f61.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12957</th>\n",
       "      <td>test\\720558d15a8fd14ae5d0301d901b58cd.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      image_name  predicted_class\n",
       "0      test\\cc27b9b56583a615fb8501e352402eb9.JPG                0\n",
       "1      test\\87872711fe672676fd34a97e997f9c47.JPG                0\n",
       "2      test\\424aa1aa8eb5bbdd07275f88077bc86c.JPG                0\n",
       "3      test\\c5537eaa60525efd7bad4a5560607e83.JPG                0\n",
       "4      test\\e9f15b67ca49453e281b2b4f245eac13.JPG                0\n",
       "...                                          ...              ...\n",
       "12953  test\\028668e733cd17ec9b9f1c7e2c657b36.JPG                0\n",
       "12954  test\\eb1f1152941fdfdd50ff9954010e622a.JPG                0\n",
       "12955  test\\bfd2dde9f4a5753c9f85b2a93bee9c03.JPG                0\n",
       "12956  test\\2eaf9c794958a93bb9984441fd5d7f61.JPG                0\n",
       "12957  test\\720558d15a8fd14ae5d0301d901b58cd.JPG                0\n",
       "\n",
       "[12958 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sample_submission.csv')\n",
    "data['image_name'] = data['image_name'].apply(lambda x: os.path.join('test', x))\n",
    "data = data[[len(np.array(PIL.Image.open(image)).shape) == 3 for image in tqdm(data['image_name'])]].reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de325b24-5368-427d-a785-c01ea4a3136b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7515b3f7d5434c8f92280adfa6cb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pixel_values'],\n",
       "    num_rows: 12958\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(data[['image_name']])\n",
    "test_dataset = test_dataset.cast_column('image_name', Image())\n",
    "test_dataset = test_dataset.map(transforms, remove_columns=['image_name']) # if enough memory\n",
    "test_dataset = test_dataset.with_format('torch') # if dataset.map\n",
    "gc.collect()\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71661cd6-af4f-4173-846a-4ac6131fdd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([4, 5, 0, ..., 5, 6, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset).predictions.argmax(-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e2b3ab-975d-466c-a514-d40cce47af69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc27b9b56583a615fb8501e352402eb9.JPG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87872711fe672676fd34a97e997f9c47.JPG</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424aa1aa8eb5bbdd07275f88077bc86c.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c5537eaa60525efd7bad4a5560607e83.JPG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e9f15b67ca49453e281b2b4f245eac13.JPG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12953</th>\n",
       "      <td>028668e733cd17ec9b9f1c7e2c657b36.JPG</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12954</th>\n",
       "      <td>eb1f1152941fdfdd50ff9954010e622a.JPG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12955</th>\n",
       "      <td>bfd2dde9f4a5753c9f85b2a93bee9c03.JPG</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>2eaf9c794958a93bb9984441fd5d7f61.JPG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12957</th>\n",
       "      <td>720558d15a8fd14ae5d0301d901b58cd.JPG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 image_name  predicted_class\n",
       "0      cc27b9b56583a615fb8501e352402eb9.JPG                4\n",
       "1      87872711fe672676fd34a97e997f9c47.JPG                5\n",
       "2      424aa1aa8eb5bbdd07275f88077bc86c.JPG                0\n",
       "3      c5537eaa60525efd7bad4a5560607e83.JPG                1\n",
       "4      e9f15b67ca49453e281b2b4f245eac13.JPG                6\n",
       "...                                     ...              ...\n",
       "12953  028668e733cd17ec9b9f1c7e2c657b36.JPG                5\n",
       "12954  eb1f1152941fdfdd50ff9954010e622a.JPG                4\n",
       "12955  bfd2dde9f4a5753c9f85b2a93bee9c03.JPG                5\n",
       "12956  2eaf9c794958a93bb9984441fd5d7f61.JPG                6\n",
       "12957  720558d15a8fd14ae5d0301d901b58cd.JPG                0\n",
       "\n",
       "[12958 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_name'] = data['image_name'].apply(os.path.basename)\n",
    "data['predicted_class'] = predictions\n",
    "data.to_csv('submission.csv', index=False)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
